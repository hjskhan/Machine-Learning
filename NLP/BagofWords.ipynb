{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "edce200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "paragraph = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting \n",
    "industry. Lorem Ipsum has been the industry's standard dummy text ever since\n",
    "the 1500s, when an unknown printer took a galley of type and scrambled it\n",
    "to make a type specimen book. It has survived not only five centuries, but\n",
    "also the leap into electronic typesetting, remaining essentially unchanged.\n",
    "It was popularised in the 1960s with the release of Letraset sheets containing\n",
    "Lorem Ipsum passages, and more recently with desktop publishing software like\n",
    "Aldus PageMaker including versions of Lorem Ipsum.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ded16ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmitizer = WordNetLemmatizer()\n",
    "\n",
    "sentences = sent_tokenize(paragraph)\n",
    "words = word_tokenize(paragraph)\n",
    "\n",
    "encoded = []\n",
    "for i in range(len(sentences)):\n",
    "    wordRe = re.sub('[^a-zA-Z]',' ',sentences[i]) # only selecting alphabets\n",
    "    wordRe = wordRe.lower() # lowering all the letters\n",
    "    wordRe = wordRe.split() # splitting the words seperately\n",
    "    wordRe = [lemmitizer.lemmatize(word) for word in wordRe if word not in stopwords.words('english')]\n",
    "    encoded.append(' '.join(wordRe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95495c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General extracted words:\n",
      "\n",
      " ['Lorem', 'Ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry', '.', 'Lorem', 'Ipsum', 'has', 'been', 'the', 'industry', \"'s\", 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s', ',', 'when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen', 'book', '.', 'It', 'has', 'survived', 'not', 'only', 'five', 'centuries', ',', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'typesetting', ',', 'remaining', 'essentially', 'unchanged', '.', 'It', 'was', 'popularised', 'in', 'the', '1960s', 'with', 'the', 'release', 'of', 'Letraset', 'sheets', 'containing', 'Lorem', 'Ipsum', 'passages', ',', 'and', 'more', 'recently', 'with', 'desktop', 'publishing', 'software', 'like', 'Aldus', 'PageMaker', 'including', 'versions', 'of', 'Lorem', 'Ipsum', '.'] \n",
      "\n",
      "\n",
      " Words after Regex,lowercase,lemmitize,stopwords:\n",
      "\n",
      " ['lorem ipsum simply dummy text printing typesetting industry', 'lorem ipsum industry standard dummy text ever since unknown printer took galley type scrambled make type specimen book', 'survived five century also leap electronic typesetting remaining essentially unchanged', 'popularised release letraset sheet containing lorem ipsum passage recently desktop publishing software like aldus pagemaker including version lorem ipsum']\n"
     ]
    }
   ],
   "source": [
    "print('General extracted words:\\n\\n',words,'\\n\\n\\n',\n",
    "      'Words after Regex,lowercase,lemmitize,stopwords:\\n\\n',encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bbc2b6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 2, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 1, 2, 0, 1, 1,\n",
       "        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of words application\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "CV = CountVectorizer()\n",
    "BgWords = CV.fit_transform(encoded).toarray()\n",
    "\n",
    "BgWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7510a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
